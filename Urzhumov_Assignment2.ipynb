{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "You may also want to implement:\n",
        "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
        "- some recent (or not very recent) paper on this topic,\n",
        "- solution which takes into account keyboard layout and associated misspellings,\n",
        "- efficiency improvement to make the solution faster,\n",
        "- any other idea of yours to improve the Norvigâ€™s solution.\n",
        "\n",
        "IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "from queue import Queue\n",
        "import nltk\n",
        "import math\n",
        "import heapq\n",
        "import random"
      ],
      "metadata": {
        "id": "JI_C6Zwvm9hH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpellChecker:\n",
        "    def __init__(self, use_keyboard=True, n_range=[2, 5], search_breadths=10):\n",
        "        \"\"\"\n",
        "        Initialization with all the preferences and keyboard matrix\n",
        "        \"\"\"\n",
        "        self.name = f\"Spell checker n-gram based model{' with keyboard mapping' if use_keyboard else ''}\"\n",
        "        self.use_keyboard = use_keyboard\n",
        "        self.keyboard_matrix = {'q': ['w', 'a'], 'w': ['q', 'a', 's', 'e'], 'e': ['w', 's', 'd', 'r'],\n",
        "                                'r': ['e', 'd', 'f', 't'], 't': ['r', 'f', 'g', 'y'], 'y': ['t', 'g', 'h', 'u'],\n",
        "                                'u': ['y', 'h', 'j', 'i'], 'i': ['u', 'j', 'k', 'o'], 'o': ['i', 'k', 'l', 'p'],\n",
        "                                'p': ['o', 'l'], 'a': ['q', 'w', 's', 'z'], 's': ['a', 'w', 'e', 'd', 'z', 'x'],\n",
        "                                'd': ['s', 'e', 'r', 'f', 'c', 'x'], 'f': ['d', 'r', 't', 'g', 'c', 'v'],\n",
        "                                'g': ['f', 't', 'y', 'h', 'v', 'b'], 'h': ['g', 'y', 'u', 'b', 'n', 'j'],\n",
        "                                'j': ['h', 'u', 'i', 'k', 'n', 'm'], 'k': ['j', 'i', 'l', 'm'], 'l': ['k', 'o', 'p'],\n",
        "                                'z': ['a', 's', 'x'], 'x': ['z', 's', 'd', 'c'], 'c': ['x', 'd', 'f', 'v'],\n",
        "                                'v': ['c', 'f', 'g', 'b'], 'b': ['v', 'g', 'h', 'n'], 'n': ['b', 'h', 'j', 'm'],\n",
        "                                'm': ['n', 'j', 'k']}\n",
        "        self.ngram_vocab = Counter()\n",
        "        self.ngram_total_by_len = dict()\n",
        "        self.words = Counter()\n",
        "        self.total_words = 0\n",
        "        self.top_k = search_breadths\n",
        "        self.n_range = n_range\n",
        "        for n in range(n_range[0], n_range[1] + 1):\n",
        "            self.ngram_total_by_len[n] = 0\n",
        "\n",
        "    def load_ngrams(self, filename):\n",
        "        \"\"\"\n",
        "        Method to load the dataset of ngrams and fill the ngram_vocab with all possible n_range-grams\n",
        "        \"\"\"\n",
        "        ngrams = []\n",
        "        with open(filename) as ngram_list:\n",
        "            ngrams = ngram_list.readlines()\n",
        "        for i in range(len(ngrams)):\n",
        "            txt = ngrams[i]\n",
        "            count, ngram = int(txt.split()[0]), txt.split()[1:]\n",
        "            for w in ngram:\n",
        "                self.words[w] += count\n",
        "                self.total_words += count\n",
        "            for _n in range(self.n_range[0], self.n_range[1] + 1):\n",
        "                window = [0, min(len(ngram), _n)]\n",
        "                while window[1] < len(ngram):\n",
        "                    self.ngram_vocab[' '.join(ngram[window[0]:window[1]])] += count\n",
        "                    self.ngram_total_by_len[_n] += count\n",
        "                    window[0] += 1\n",
        "                    window[1] += 1\n",
        "                if len(ngram) == _n:\n",
        "                    break\n",
        "\n",
        "    def ngram_score(self, ngram):\n",
        "        \"\"\"\n",
        "        Method to calculate total ngram score. This score is used to rank the candidate sentences after the correction.\n",
        "        Ngram score is calculated by the formula: n^2 * (sum of frequencies (in the loaded ngrams data) of all n-grams found in the sentence)\n",
        "        for all integer n in n_range\n",
        "        \"\"\"\n",
        "        if not ngram:\n",
        "            return 0\n",
        "        wordset = ngram.split() if type(ngram) == str else ngram\n",
        "        score = 0\n",
        "        subsets = [' '.join(wordset)]\n",
        "        n = len(wordset)\n",
        "        while n >= self.n_range[0]:\n",
        "            subsets = []\n",
        "            n -= 1\n",
        "            window = [0, n]\n",
        "            while window[1] < len(wordset):\n",
        "                subsets.append(' '.join(wordset[window[0]:window[1]]))\n",
        "                window[0] += 1\n",
        "                window[1] += 1\n",
        "            score += max(1, n ** 2) * sum([self.ngram_vocab[subset] / self.ngram_total_by_len[n] if subset in\n",
        "                                          self.ngram_vocab else 0 for subset in subsets])\n",
        "        return score\n",
        "\n",
        "    def word_prob(self, word):\n",
        "        \"\"\"\n",
        "        Method to get a word probability (frequency in the loaded dataset), same as P calculation in Norwig's solution\n",
        "        \"\"\"\n",
        "        return self.words[word] / self.total_words\n",
        "\n",
        "    def known(self, words):\n",
        "        return set(w for w in words if w in self.words)\n",
        "\n",
        "    def candidates(self, word):\n",
        "        return (self.known([word]) or self.known(self.edits1(word)) or self.known(self.edits2(word)) or [word])\n",
        "\n",
        "    def edits1(self, word):\n",
        "        letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "        splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "        deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "        replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "        inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "        return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "    def edits2(self, word):\n",
        "        return (e2 for e1 in self.edits1(word) for e2 in self.edits1(e1))\n",
        "\n",
        "    def correct_word(self, word):\n",
        "        results = self.known([word])\n",
        "\n",
        "        if len(results) == 0:\n",
        "            results = results.union(self.known(self.edits1(word)))\n",
        "            results = results.union(self.known(self.edits2(word)))\n",
        "\n",
        "        if len(results) > 0:\n",
        "            edit_distances = dict()\n",
        "            for corrected_word in results:\n",
        "                dist = self.edit_distance(corrected_word, word)\n",
        "                edit_distances[dist] = edit_distances.get(dist, [])\n",
        "                edit_distances[dist].append(corrected_word)\n",
        "            sorted_dist = sorted(list(edit_distances.keys()))\n",
        "            score = []\n",
        "            for d, ws in edit_distances.items():\n",
        "                score.extend((w, self.words[w] * 1e5 / (self.total_words * max(1, d ** 4))) for w in ws)\n",
        "            top_k_options = heapq.nlargest(self.top_k, score, key=lambda x: x[1])\n",
        "            top_k_words = [option[0] for option in top_k_options]\n",
        "            return top_k_words\n",
        "        else:\n",
        "            return [word]\n",
        "\n",
        "    def correct_sentence(self, sentence):\n",
        "        sentence = list(map(lambda x: x.lower(), sentence.split() if type(sentence) == str else sentence))\n",
        "        options = []\n",
        "        corrected_sentences = self.generate_corrected_sentences(sentence)\n",
        "        for corrected_sentence in corrected_sentences:\n",
        "            if len(corrected_sentence) == len(sentence):\n",
        "                total_score = self.ngram_score(corrected_sentence)\n",
        "                options.append((total_score, corrected_sentence))\n",
        "        options.sort(key=lambda x: x[0], reverse=True)\n",
        "        return ' '.join(options[0][1])\n",
        "\n",
        "    def generate_corrected_sentences(self, words):\n",
        "        if not words:\n",
        "            return [[]]\n",
        "\n",
        "        rest_sentences = self.generate_corrected_sentences(words[1:])\n",
        "        first_word_sentences = self.correct_word(words[0])\n",
        "        corrected_sentences = []\n",
        "        for first_word in first_word_sentences:\n",
        "            for rest_sentence in rest_sentences:\n",
        "                corrected_sentences.append([first_word] + rest_sentence)\n",
        "        return corrected_sentences\n",
        "\n",
        "    def edit_distance(self, corrected_word, original_word):\n",
        "        \"\"\"\n",
        "        Method to calculate the edit distance between the original and corrected words.\n",
        "        Reuses code from the Spell_checker by Almaz Samatov, provided in the explanation section\n",
        "        \"\"\"\n",
        "        def get_distance_between_letters(letter1, letter2):\n",
        "            \"\"\"\n",
        "            Nested function for calculation of distance between the given letters.\n",
        "            Reuses code from the Spell_checker by Almaz Samatov, provided in the explanation section\n",
        "            \"\"\"\n",
        "            q = Queue()\n",
        "            q.put(letter1)\n",
        "            used = set()\n",
        "            used.add(letter1)\n",
        "            dist = dict()\n",
        "            dist[letter1] = 0\n",
        "            while not q.empty():\n",
        "                symbol = q.get()\n",
        "                for symbol_to_go in self.keyboard_matrix[symbol]:\n",
        "                    if symbol_to_go not in used:\n",
        "                        used.add(symbol_to_go)\n",
        "                        q.put(symbol_to_go)\n",
        "                        dist[symbol_to_go] = dist[symbol] + 1\n",
        "            if dist[letter2] <= 2:\n",
        "                return dist[letter2]\n",
        "            else:\n",
        "                return 3\n",
        "\n",
        "        distance = [[0 for i in range(len(corrected_word) + 1)] for j in range(len(original_word) + 1)]\n",
        "\n",
        "        for i in range(len(original_word) + 1):\n",
        "            for j in range(len(corrected_word) + 1):\n",
        "                if min(i, j) == 0:\n",
        "                    distance[i][j] = max(i, j)\n",
        "                else:\n",
        "                    deletion_cost = distance[i - 1][j] + 2\n",
        "\n",
        "                    if corrected_word[j - 1] == original_word[i - 1]:\n",
        "                        insertion_cost = distance[i][j - 1] + 1\n",
        "                    else:\n",
        "                        insertion_cost = distance[i][j - 1] + 3\n",
        "\n",
        "                    substitution_cost = distance[i - 1][j - 1] + get_distance_between_letters(original_word[i - 1],\n",
        "                                                                                              corrected_word[j - 1])\n",
        "\n",
        "                    if i > 1 and j > 1 and corrected_word[j - 1] == original_word[i - 2] \\\n",
        "                            and corrected_word[j - 2] == original_word[i - 1]:\n",
        "                        transposition_cost = distance[i - 2][j - 2] + 1\n",
        "                        distance[i][j] = min(deletion_cost, insertion_cost, substitution_cost, transposition_cost)\n",
        "                    else:\n",
        "                        distance[i][j] = min(deletion_cost, insertion_cost, substitution_cost)\n",
        "\n",
        "        return distance[len(original_word)][len(corrected_word)]\n",
        "\n",
        "\n",
        "    def deform_word(self, word, distance_probs={0: 0.4, 1: 0.3, 2: 0.2, 3: 0.05, 4: 0.05}):\n",
        "        \"\"\"\n",
        "        Method that reuses the methods from the above to deform the word\n",
        "        accordingly to the distance probabilities given.\n",
        "        Uses some aproaches to lower the time needed to deform one word with a good quality\n",
        "        \"\"\"\n",
        "        num = random.random()\n",
        "        if 0 in distance_probs and num <= distance_probs[0]:\n",
        "            return word\n",
        "        ed1s = list(self.edits1(word))\n",
        "        random.shuffle(ed1s)\n",
        "        deformed = [word] + ed1s[:min(len(ed1s), max(10, int(0.025 * len(ed1s))))]\n",
        "\n",
        "        guaranteed_one_edit_prob = distance_probs[0] if 0 in distance_probs else 0 + \\\n",
        "                                   distance_probs[1] if 1 in distance_probs else 0\n",
        "        if num > guaranteed_one_edit_prob:\n",
        "            ed2s = list(self.edits2(word))\n",
        "            random.shuffle(ed2s)\n",
        "            deformed.extend(ed2s[:min(len(ed2s), 10)])\n",
        "\n",
        "        edit_distances = dict()\n",
        "        for w in deformed:\n",
        "            dist = self.edit_distance(w, word)\n",
        "            edit_distances[dist] = edit_distances.get(dist, [])\n",
        "            edit_distances[dist].append(w)\n",
        "        sorted_dist = sorted(list(edit_distances.keys()))\n",
        "\n",
        "        candidates = []\n",
        "        for item in distance_probs.items():\n",
        "            pass\n",
        "\n",
        "        cumulative_probs = [0]\n",
        "        for dist, prob in distance_probs.items():\n",
        "            cumulative_probs.append(cumulative_probs[-1] + prob)\n",
        "\n",
        "        for i in range(len(cumulative_probs) - 1):\n",
        "            if cumulative_probs[i] <= num < cumulative_probs[i + 1]:\n",
        "                if len(sorted_dist) <= i:\n",
        "                    i = len(sorted_dist) - 1\n",
        "                target_dist = sorted_dist[i]\n",
        "                break\n",
        "\n",
        "        if target_dist in edit_distances:\n",
        "            return random.choice(edit_distances[target_dist])\n",
        "        else:\n",
        "            return word"
      ],
      "metadata": {
        "id": "lhzEpWwbnAjz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spc = SpellChecker()  # My spellchecker is easy to use. Just init, load_ngrams and then you are ready to go!\n",
        "spc.load_ngrams('fivegrams.txt')  # I've loaded fivegrams to allow usage of all ngrams in n_range=[2, 5]"
      ],
      "metadata": {
        "id": "FuNzN7oKmQrP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Some examples to figure out the possibilities\n",
        "'''\n",
        "print('Bib af reah hollwo  --> ', spc.correct_sentence('Bib af reah hollwo'))\n",
        "print('Thib is an exampel sntrnce  --> ', spc.correct_sentence('Thib is an exampel sntrnce'))\n",
        "print('tell me eqy I fillow youu  --> ', spc.correct_sentence('tell me eqy I fillow youu'))\n",
        "print('a bbleu sea is far awqy frpm me  --> ', spc.correct_sentence('a bbleu sea is far awqy frpm me'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HwVsWNW-1kf",
        "outputId": "2113dfe7-3ff8-45ad-a408-207b1e8140a0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bib af reah hollwo  -->  it was great hollow\n",
            "Thib is an exampel sntrnce  -->  this is an example sentence\n",
            "tell me eqy I fillow youu  -->  tell me way i follow you\n",
            "a bbleu sea is far awqy frpm me  -->  a blue sea is far away from me\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Here is the vanilla code of Norwig's solution for sentence correction\n",
        "'''\n",
        "\n",
        "\n",
        "def words(text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "WORDS = Counter(words(open('fivegrams.txt').read()))\n",
        "\n",
        "def P(word, N=sum(WORDS.values())):\n",
        "    \"Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def correction(word):\n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word):\n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words):\n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word):\n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "def norwig_correct_sentence(sentence):\n",
        "    words_involved = list(map(lambda x: x.lower(), sentence.split()))\n",
        "    for i, w in enumerate(words_involved):\n",
        "        words_involved[i] = correction(w)\n",
        "    return ' '.join(words_involved)"
      ],
      "metadata": {
        "id": "WZWxKrjpV7FB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(norwig_correct_sentence('Bib af reah hollwo'))\n",
        "print(norwig_correct_sentence('Thib is an exampel sntrnce'))\n",
        "print(norwig_correct_sentence('tell me eqy I fillow youu'))\n",
        "print(norwig_correct_sentence('a bbleu sea is far awqy frpm me'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_jKRTZAPMqP",
        "outputId": "0e35dc9b-49c2-4032-eb78-9b075e333c92"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "big of real hollow\n",
            "this is an example entrance\n",
            "tell me way i follow you\n",
            "a able sea is far away from me\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xb_twOmVsC6"
      },
      "source": [
        "For more context, I have used 5grams dataset including all the (2, 5)grams (from bigrams to fivegrams). One may change the hyperparameter (even to larger n if one is able to find dataset for ngrams) to consider other gram sizes\n",
        "\n",
        "Weights are assigned accordingly to the Damerau-Levenshtein edit distance between corrected and original word, taking into account the neighboring words on the keyboard.\n",
        "For example, word 'now' will have less edit distance from the misspelled word 'noq' than the word 'not'.\n",
        "\n",
        "All the weights are applied on different stages of candidates preparation or the exact candidate choice:\n",
        "\n",
        "\n",
        "*   edit distance is affecting the stage of word correction, which is made before the candidate sentences choice (candidate sentences are all sentences with correct words which can be constructed by 0-2 edits of the initial ones, even if the initial words are present in the vocabulary). Note that two misclicked letters which are close on the keyboard to the corrected ones can give lower distance than one misclick from far away.\n",
        "*   word probability P is applied at the same place where edit distance is applied (so the weight is the following: 1e5 * P/(edit_distance**4))\n",
        "*   ngram probability is calculated during the final choice among the candidate sentences. It takes into account all the (2, 5)grams (or, again, another predefined by a parameter gram set), and the more words in the ngram that is present in the sentence, the bigger score it will get and will have bigger probability to be chosen\n",
        "\n",
        "Beam search here is altered by the techniques described above (not a direct vanilla beam-search, but candidate filtering).\n",
        "\n",
        "\n",
        "The ideas, implementation parts, concept mappings, namings, etc. were taken from different sources to present a unique self-implemented solution. As references, I would like to put the following:\n",
        "\n",
        "[Norwig's solution](https://norvig.com/spell-correct.html) that is a baseline was also used as a base code for the above solution;\n",
        "\n",
        "[Spelling correction guide](https://towardsdatascience.com/spelling-correction-how-to-make-an-accurate-and-fast-corrector-dc6d0bcbba5f) by Filipp Bakanov with awesome ideas how to iteratively improve the Norwig's solution, with quantitative and qualitative analysis. Here I've taken the idea of how to use ngrams. One can find here further improvements and next steps;\n",
        "\n",
        "[Spell_checker](https://github.com/AlmazSamatov/Spell_checker/tree/master) by Almaz Samatov, a solution to introduce keyboard graph and edit distance over the Norwig's solution. I've taken the edit distance and keyboard mapping dictionary from it to avoid implementing it from scratch;\n",
        "\n",
        "[SymSpell](https://wolfgarbe.medium.com/1000x-faster-spelling-correction-algorithm-2012-8701fcd87a5f) have inspired me on how to reduce the number of computations (but I am far away from 1000x faster correction presented there)\n",
        "\n",
        "[Yandex Cup on ML](https://yandex.ru/cup/ml/) with its task of NeuroSwipe on the keyboard, which I was solving in 2023, helped me to outline significant points of how to weight up the distance calculated and the word probability in the dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noise probability is used in the deform_word method in another form: a probability of edit_distance between the word and deformed one"
      ],
      "metadata": {
        "id": "ea13kkP66YhN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OwZWaX9VVs7B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abc83695-0bc8-42b9-e8e7-f7f9dea40588"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import nltk\n",
        "from tqdm.notebook import tqdm\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "\n",
        "with open('big.txt') as big:\n",
        "    big_s = big.readlines()\n",
        "\n",
        "text = ' '.join(big_s)\n",
        "sentences = sent_tokenize(text)\n",
        "random.shuffle(sentences)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = sentences[0].split()[0]\n",
        "print(w.lower(), spc.deform_word(w.lower(), {0: 0.1, 1: 0.2, 2: 0.3, 3: 0.4}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd3140Hxv1xq",
        "outputId": "0d3e1d21-6552-4d83-c17a-0e38cb3bc59a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "muttering muytering\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_SAMPLES = 1000\n",
        "norwig_total_w = 0\n",
        "spc_total_w = 0\n",
        "total_words = 0\n",
        "norwig_total_s = 0\n",
        "spc_total_s = 0\n",
        "\n",
        "\n",
        "for s in tqdm(sentences[:TEST_SAMPLES]):\n",
        "    orig_words = []\n",
        "    sent_deformation = []\n",
        "    for w in s.split():\n",
        "        w = w.lower()\n",
        "        temp_w = ''\n",
        "        for c in w:\n",
        "            if ord('a') <= ord(c) <= ord('z'):\n",
        "                temp_w += c\n",
        "        pref_symb, suf_symb = '', ''\n",
        "        if ord('a') <= ord(w[0]) <= ord('z'):\n",
        "            pref_symb = w[0]\n",
        "        if ord('a') <= ord(w[-1]) <= ord('z'):\n",
        "            suf_symb = w[-1]\n",
        "        w = temp_w\n",
        "        orig_words.append(w)\n",
        "        new_w = spc.deform_word(w)\n",
        "        sent_deformation.append(new_w)\n",
        "        total_words += 1\n",
        "    deformed_sentence = ' '.join(sent_deformation)\n",
        "    norwigs_res = norwig_correct_sentence(deformed_sentence)\n",
        "    spc_res = spc.correct_sentence(deformed_sentence)\n",
        "\n",
        "    for i, w in enumerate(norwigs_res.split()):\n",
        "        if w == orig_words[i]:\n",
        "            norwig_total_w += 1\n",
        "    if ' '.join(orig_words) == norwigs_res:\n",
        "        norwig_total_s += 1\n",
        "\n",
        "    for i, w in enumerate(spc_res.split()):\n",
        "        if w == orig_words[i]:\n",
        "            spc_total_w += 1\n",
        "    if ' '.join(orig_words) == spc_res:\n",
        "        spc_total_s += 1"
      ],
      "metadata": {
        "id": "P3ezF3n2GDr3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a1f24b77a0714ab09df366fa04e8e8dc",
            "1544304af1d64606b5483d92d8e74695",
            "a0274a8285d743f4b274236fd561613c",
            "af959410fdc14a1da973c20b70c505f5",
            "23abc0f1ff114862882c24ce363b0449",
            "d011b490e12d427b9e5458999738b0ae",
            "2f246507d4684bc1939b59a9043c9f7c",
            "c8736bbdd28e496a9ab2391ec97ea108",
            "ce84c953cc85490782ce51ba0eca803e",
            "cb91192d5f7947b6b25c0636d5feed94",
            "1ffd0303e9944d4392a68973f4459416"
          ]
        },
        "outputId": "ae5570b4-485e-4ab2-d7bd-033eca6ee83b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1f24b77a0714ab09df366fa04e8e8dc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Per-word accuracies: \\nNorwig: {norwig_total_w / total_words} \\nSpellChecker: {spc_total_w / total_words}')\n",
        "print(f'Per-sentence accuracies: \\nNorwig: {norwig_total_s / TEST_SAMPLES}\\nSpellChecker: {spc_total_s / TEST_SAMPLES}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mkKs4jJ2-Fw",
        "outputId": "89a90c9f-4977-49be-a586-9cf5ad612573"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Per-word accuracies: \n",
            "Norwig: 0.7813664596273292 \n",
            "SpellChecker: 0.8248447204968944\n",
            "Per-sentence accuracies: \n",
            "Norwig: 0.608\n",
            "SpellChecker: 0.61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For further work\n",
        "I'd like to consider the improvements from Filipp Bakanov's guide, and also reconsider weights and score calculations to improve the performance. But overall, it has a good enough accuracy!\n",
        "\n",
        "Thanks for the assignment!"
      ],
      "metadata": {
        "id": "7yArioML67Jp"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a1f24b77a0714ab09df366fa04e8e8dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1544304af1d64606b5483d92d8e74695",
              "IPY_MODEL_a0274a8285d743f4b274236fd561613c",
              "IPY_MODEL_af959410fdc14a1da973c20b70c505f5"
            ],
            "layout": "IPY_MODEL_23abc0f1ff114862882c24ce363b0449"
          }
        },
        "1544304af1d64606b5483d92d8e74695": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d011b490e12d427b9e5458999738b0ae",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2f246507d4684bc1939b59a9043c9f7c",
            "value": "100%"
          }
        },
        "a0274a8285d743f4b274236fd561613c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8736bbdd28e496a9ab2391ec97ea108",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ce84c953cc85490782ce51ba0eca803e",
            "value": 1000
          }
        },
        "af959410fdc14a1da973c20b70c505f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb91192d5f7947b6b25c0636d5feed94",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1ffd0303e9944d4392a68973f4459416",
            "value": "â€‡1000/1000â€‡[03:18&lt;00:00,â€‡â€‡3.23it/s]"
          }
        },
        "23abc0f1ff114862882c24ce363b0449": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d011b490e12d427b9e5458999738b0ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f246507d4684bc1939b59a9043c9f7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8736bbdd28e496a9ab2391ec97ea108": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce84c953cc85490782ce51ba0eca803e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb91192d5f7947b6b25c0636d5feed94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ffd0303e9944d4392a68973f4459416": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}